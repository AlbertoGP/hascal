// The Hascal Lexer
//
// wikipedia :
// In computer science, lexical analysis, lexing or tokenization is the process of 
// converting a sequence of characters (such as in a computer program or web page) into 
// a sequence of _tokens (strings with an assigned and thus identified meaning). 
// A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner,
// although scanner is also a term for the first stage of a lexer. 
// A lexer is generally combined with a parser, which together analyze the syntax of programming languages, 
// web pages, and so forth.

use strings
local use keywords

// token struct
struct token {
    var type : string
    var value : string
    var line : int
}

var _lexme = "" // current lexeme
var current = 0 // current character
var _source = "" // source code
var line = 1 // current line

var _tokens : [token]

// this function is used to check if the current character is at the end of the string
function isAtEnd() : bool {
    if current >= len(_source) {
        return true
    }
    return false
}

function advance() : char{
    current = current + 1
    return _source[current]
}

function scan_token(){
    var c = advance()
    var tok : token

    if c == ' ' {
        scan_token()
    } else if c == '\n' {
        tok = token("NEWLINE", "", line)
        append(_tokens,tok)
        line = line + 1
    } else if c == '(' {
        tok = ["LPAREN","(",line]
        append(_tokens,tok)
    } else if c == ')' {
        tok = ["RPAREN",")",line]
        append(_tokens,tok)
    } else if is_alpha(c) or c == '_' {
        _lexme = to_string(c)
        while not isAtEnd() and (is_alpha(c) or is_number(c) or c == '_') {
            _lexme = _lexme + to_string(c)
            c = advance()
        }

        if check_keyword(_lexme) {
            tok = token(_lexme,_lexme,line)
        } else {
            tok = token("NAME",_lexme,line)
        }
    } else if is_number(c) {
        _lexme = to_string(c)
        while not isAtEnd() and is_number(c) {
            _lexme = _lexme + to_string(c)
            c = advance()
        }
        tok = token("NUMBER", _lexme, line)
        append(_tokens,tok)
    } else if c == '"' {
        _lexme = to_string(c)
        while not isAtEnd() and c != '"' {
            _lexme = _lexme + to_string(c)
            c = advance()
        }
        c = advance()
        tok = token("STRING", _lexme, line)
        append(_tokens,tok)
    } else if c == '+' {
        tok = token("PLUS", "+", line)
        append(_tokens,tok)
    } else if c == '-' {
        tok = token("MINUS", "-", line)
        append(_tokens,tok)
    } else if c == '*' {
        tok = token("STAR", "*", line)
        append(_tokens,tok)
    } else if c == '/' {
        tok = token("SLASH", "/", line)
        append(_tokens,tok)
    } else if c == '%' {
        tok = token("PERCENT", "%", line)
        append(_tokens,tok)
    } else if c == '^' {
        tok = token("CARET", "^", line)
        append(_tokens,tok)
    } else if c == '<' {
        c = advance()
        if c == '=' {
            tok = token("LTE", "<=", line)
            append(_tokens,tok)
        } else {
            tok = token("LT","<",line)
            append(_tokens,tok)
        }
    }
}
function tokenizer(src:string): [token] {
    _source = src
    while isAtEnd() == false {
        scan_token()
    }
    return _tokens
}
